units:
  endtime_s: [hr]
  forcing_resolution_h: [ sec ]
  timestep_h: [s]
subcycle_length: 3600 #[sec]
subcycle_length_h: None #[hr]
num_subcycles: None
#num_time_step: 90000  # total simulation time is this number times the time_step_s
#endtime: 372.0 #[hr]
endtime: 3000.0 #[hr]
endtime_s: None
forcing_resolution_h: None
time_per_step: None
nsteps: None
forcing_resolution: 3600 # [sec]
hyperparameters:
  warmup: 168
#  minibatch: 31 # [days]
#  minibatch: 15 # [days]
  minibatch: 0.04166666667 # this makes no minibatch
  lb:
    - 0.0015 # alpha
    - 1.0 # n
    - 1e-6 # Ksat
    - 0.001 # ponded_depth_max
  ub:
    - 0.015 # alpha
    - 5.0 # n
    - 30 # Ksat
    - 10.0 # ponded_depth_max
  # https://www.pc-progress.com/Downloads/Public_Lib_H1D/HYDRUS-1D_Tutorial_V1.00_2018.pdf
  # https://acsess.onlinelibrary.wiley.com/doi/pdf/10.2136/sssaj1980.03615995004400050002x
# -----------------------
# NH variables are below
# -----------------------
epochs: 50
learning_rate: 0.001
model: cudalstm
head: regression
output_activation: linear
statics_embedding:
  type: fc
  # define number of neurons per layer in the FC network used as embedding network
  hiddens:
    - 30
    - 20
    - 64
  # activation function of embedding network
  activation: tanh
  # dropout applied to embedding network
  dropout: 0.0

target_variables:
  - max_ponded_depth
  - alpha
  - n
  - ksat
  - theta_e
  - theta_r

# define embedding network for dynamic inputs
dynamics_embedding:
  type: fc
  # define number of neurons per layer in the FC network used as embedding network
  hiddens:
    - 30
    - 20
    - 64
  # activation function of embedding network
  activation: tanh
  # dropout applied to embedding network
  dropout: 0.0

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 64

# Initial bias value of the forget gate
initial_forget_bias: 3

# Dropout applied to the output of the LSTM
output_dropout: 0.4
