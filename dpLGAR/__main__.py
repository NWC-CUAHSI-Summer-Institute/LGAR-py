import hydra
import logging
from omegaconf import DictConfig
import os
import time

# from dpLGAR.training.DataParallelLGAR import DataParallelLGAR
# from dpLGAR.training.SingleBasinRun import LGARAgent
from dpLGAR.training.basetrainer import BaseTrainer
# from dpLGAR.training.SyntheticAgent import SyntheticAgent


log = logging.getLogger(__name__)


# @hydra.main(version_base=None, config_path=".", config_name="config")
# def main_old(cfg: DictConfig) -> None:
#     start = time.perf_counter()
#     try:
#         # Running the code in Parallel
#         # Pulling the LOCAL_RANK that is generated by torchrun
#         cfg.local_rank = int(os.environ["LOCAL_RANK"])
#         agent = DataParallelLGAR(cfg)
#     except KeyError:
#         # There is no Data Parallel in use
#         cfg.local_rank = 0
#         cfg.nproc = 1
#         if cfg.synthetic.eval:
#             agent = SyntheticAgent(cfg)
#         else:
#             agent = LGARAgent(cfg)
#
#     agent.run()
#     agent.finalize()
#     end = time.perf_counter()
#     log.debug(f"Run took : {(end - start):.6f} seconds")
#
@hydra.main(version_base=None, config_path=".", config_name="config")
def main(cfg: DictConfig) -> None:
    start = time.perf_counter()
    if cfg.mode == "train":
        start_train(cfg)
    end = time.perf_counter()


def start_train(cfg: DictConfig) -> None:
    trainer = BaseTrainer(cfg)
    trainer.initialize_training()



if __name__ == "__main__":
    main()
